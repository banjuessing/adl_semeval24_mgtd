# Leveraging (Sentence) Transformer Models with Contrastive Learning for Identifying Machine-Generated Text
## Overview
This is the code base of the project for Applied Deep Learning WS23/24 & SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection.

Our detection system is built upon Transformer-based techniques, leveraging various pre-trained language models (PLMs), including sentence transformer models. Additionally, we incorporate Contrastive Learning (CL) into the classifier to improve the detecting capabilities and employ Data Augmentation methods. Ultimately, our system achieves a peak accuracy of 76.96% on the test set of the competition, configured using a sentence transformer model integrated with CL methodology.

## Quick Links
| Section                                 | Description                                                     |
| :-------------------------------------: |:--------------------------------------------------------------: |
| [Requirements](#requirements)          | How to set up the python environemnt of our experiments         |
| [Data Preparation](#data Preparation)  | How to download and prepare the data for our experiments        |
| [Experiments](#experiments)            | How to run our experiments and use our best model for inference |

## Requirements

## Data Preparation

## Experiments
