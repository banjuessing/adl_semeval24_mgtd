{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v4X8Nn2i28Qe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggDk0GiA3Hs8",
        "outputId": "94d9f11f-85d8-45be-c3fa-e4902fef1922"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MGTD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO64R6_J3KcV",
        "outputId": "5fcf1940-160c-4ee0-ca98-6b601802e9f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MGTD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files as dataframes\n",
        "train_df = pd.read_json(\"./data/subtaskB_train.jsonl\", lines=True)\n",
        "dev_df = pd.read_json(\"./data/subtaskB_dev.jsonl\", lines=True)"
      ],
      "metadata": {
        "id": "yvObeVKr3jEr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sources and labels\n",
        "sources = ['wikihow', 'reddit', 'arxiv', 'wikipedia']\n",
        "labels = ['human', 'chatGPT', 'cohere', 'bloomz', 'davinci', 'dolly']"
      ],
      "metadata": {
        "id": "YNihOyMk3pqj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_move_samples(train_df, dev_df, source, label, num_samples):\n",
        "    \"\"\"\n",
        "    For each specified source (wikihow, reddit, arxiv, wikipedia), extract 3000 samples.\n",
        "    Ensure that within these 3000 samples from each source, there are equal numbers (500) of each label (human, chatGPT, cohere, bloomz, davinci, dolly).\n",
        "    Move these samples from the training set to the development set.\n",
        "    \"\"\"\n",
        "    # Extract samples\n",
        "    samples = train_df[(train_df['source'] == source) & (train_df['model'] == label)].sample(n=num_samples, random_state=42)\n",
        "\n",
        "    # Remove samples from train and add to dev\n",
        "    train_df = train_df.drop(samples.index)\n",
        "    dev_df = pd.concat([dev_df, samples], ignore_index=True)\n",
        "\n",
        "    return train_df, dev_df"
      ],
      "metadata": {
        "id": "XsB8dfj-3xus"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move equally distributed data samples from original train set to dev set\n",
        "for source in sources:\n",
        "    for label in labels:\n",
        "        train_df, dev_df = extract_and_move_samples(train_df, dev_df, source, label, 500)"
      ],
      "metadata": {
        "id": "1_noQjnrniEe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the new dev set into validation set and test set\n",
        "dev_df['stratify_col'] = dev_df['model'].astype(str) + \"_\" + dev_df['source'].astype(str)\n",
        "val_df, test_df = train_test_split(dev_df, stratify=dev_df['stratify_col'], test_size=0.5, random_state=42)\n",
        "\n",
        "# Drop 'stratify_col' from both dataframes, no longer needed\n",
        "val_df = val_df.drop('stratify_col', axis=1)\n",
        "test_df = test_df.drop('stratify_col', axis=1)"
      ],
      "metadata": {
        "id": "44pEQedZoevq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the datasets\n",
        "print(\"Shape of train_df:\", train_df.shape)\n",
        "print(\"Shape of val_df:\", val_df.shape)\n",
        "print(\"Shape of test_df:\", test_df.shape)\n",
        "\n",
        "# Check the distribution of the model classes in the train, validation and test sets\n",
        "print(\"Train set class distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
        "print(\"Validation set class distribution:\\n\", val_df['label'].value_counts(normalize=True))\n",
        "print(\"Test set class distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
        "\n",
        "# Check the distribution of the source classes in the train, validation and test sets\n",
        "print(\"Train set source distribution:\\n\", train_df['source'].value_counts(normalize=True))\n",
        "print(\"Validation set source distribution:\\n\", val_df['source'].value_counts(normalize=True))\n",
        "print(\"Test set source distribution:\\n\", test_df['source'].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzce5jj-uIyo",
        "outputId": "406870bc-9663-4bb5-af3e-6897c2e70ffd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_df: (59027, 5)\n",
            "Shape of val_df: (7500, 5)\n",
            "Shape of test_df: (7500, 5)\n",
            "Train set class distribution:\n",
            " 3    0.169397\n",
            "4    0.169380\n",
            "0    0.169363\n",
            "1    0.169329\n",
            "5    0.164365\n",
            "2    0.158165\n",
            "Name: label, dtype: float64\n",
            "Validation set class distribution:\n",
            " 2    0.166667\n",
            "0    0.166667\n",
            "1    0.166667\n",
            "4    0.166667\n",
            "5    0.166667\n",
            "3    0.166667\n",
            "Name: label, dtype: float64\n",
            "Test set class distribution:\n",
            " 3    0.166667\n",
            "0    0.166667\n",
            "2    0.166667\n",
            "4    0.166667\n",
            "1    0.166667\n",
            "5    0.166667\n",
            "Name: label, dtype: float64\n",
            "Train set source distribution:\n",
            " wikihow      0.254104\n",
            "reddit       0.254104\n",
            "arxiv        0.254070\n",
            "wikipedia    0.237722\n",
            "Name: source, dtype: float64\n",
            "Validation set source distribution:\n",
            " reddit       0.2\n",
            "wikipedia    0.2\n",
            "arxiv        0.2\n",
            "peerread     0.2\n",
            "wikihow      0.2\n",
            "Name: source, dtype: float64\n",
            "Test set source distribution:\n",
            " arxiv        0.2\n",
            "wikihow      0.2\n",
            "wikipedia    0.2\n",
            "peerread     0.2\n",
            "reddit       0.2\n",
            "Name: source, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the new splits (train:val:test = 80:10:10, label and source all equally distributed)\n",
        "train_df.to_json(\"./data/updated_subtaskB_train.jsonl\", orient='records', lines=True)\n",
        "val_df.to_json(\"./data/updated_subtaskB_validation.jsonl\", orient='records', lines=True)\n",
        "test_df.to_json(\"./data/updated_subtaskB_test.jsonl\", orient='records', lines=True)"
      ],
      "metadata": {
        "id": "d1gVo0FnunZJ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}