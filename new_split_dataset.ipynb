{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new dataset splits which can be also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided dataset files\n",
    "train_file_path = \"F:\\\\applied_deep_learning\\\\data\\\\SemEval2024-Task8\\\\SubtaskB\\\\subtaskB_train.jsonl\"\n",
    "dev_file_path = \"F:\\\\applied_deep_learning\\\\data\\\\SemEval2024-Task8\\\\SubtaskB\\\\subtaskB_dev.jsonl\"\n",
    "\n",
    "# reading the into pandas dataframe\n",
    "train_df = pd.read_json(train_file_path, lines=True)\n",
    "dev_df = pd.read_json(dev_file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words above 512 in train_df: 9985\n",
      "Highest word count in train_df: 16991\n"
     ]
    }
   ],
   "source": [
    "# find the row with the highest word count in train_df\n",
    "max_word_count_row_train = train_df['text'].apply(lambda x: len(x.split())).idxmax()\n",
    "\n",
    "# get the word count of the row in train_df\n",
    "max_word_count_train = len(train_df['text'][max_word_count_row_train].split())\n",
    "\n",
    "# find text with a token size above 512 and count it in train_df\n",
    "words_above_train = train_df['text'].apply(lambda x: len(x.split())).loc[train_df['text'].apply(lambda x: len(x.split())) > 512].count()\n",
    "\n",
    "# print the results for train_df\n",
    "print(\"Number of words above 512 in train_df:\", words_above_train)\n",
    "print(\"Highest word count in train_df:\", max_word_count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words above 512: 188\n",
      "Highest word count: 1208\n"
     ]
    }
   ],
   "source": [
    "# find the row with the highest word count\n",
    "max_word_count_row = dev_df['text'].apply(lambda x: len(x.split())).idxmax()\n",
    "\n",
    "# get the word count of the row\n",
    "max_word_count = len(dev_df['text'][max_word_count_row].split())\n",
    "\n",
    "# find text with a token size above 512 and count it\n",
    "words_above = dev_df['text'].apply(lambda x: len(x.split())).loc[dev_df['text'].apply(lambda x: len(x.split())) > 512].count()\n",
    "\n",
    "# print\n",
    "print(\"Number of words above 512:\", words_above)\n",
    "\n",
    "# print the count\n",
    "print(\"Highest word count:\", max_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(file_path):\n",
    "    # read the json into dataframe\n",
    "    df_jsonl = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # explode 'human_text' and assign 'human' to the 'model' column\n",
    "    df_human = df_jsonl[['human_text']].explode('human_text')\n",
    "    df_human.rename(columns={'human_text': 'text'}, inplace=True)\n",
    "    df_human['model'] = 'human'\n",
    "\n",
    "    # explode 'machine_text' and assign corresponding 'model' value\n",
    "    df_machine = df_jsonl[['machine_text', 'model']].explode('machine_text')\n",
    "    df_machine.rename(columns={'machine_text': 'text'}, inplace=True)\n",
    "\n",
    "    # combine the two DataFrames\n",
    "    df_combined = pd.concat([df_human, df_machine], ignore_index=True)\n",
    "\n",
    "    # drop rows where 'text' is NaN (if any)\n",
    "    df_combined.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the file paths\n",
    "file_paths = [\n",
    "    \"F:\\\\applied_deep_learning\\\\data\\\\original_M4_data\\\\peerread_bloomz.jsonl\",\n",
    "    \"F:\\\\applied_deep_learning\\\\data\\\\original_M4_data\\\\peerread_chatgpt.jsonl\",\n",
    "    \"F:\\\\applied_deep_learning\\\\data\\\\original_M4_data\\\\peerread_cohere.jsonl\",\n",
    "    \"F:\\\\applied_deep_learning\\\\data\\\\original_M4_data\\\\peerread_davinci.jsonl\",\n",
    "    \"F:\\\\applied_deep_learning\\\\data\\\\original_M4_data\\\\peerread_dolly.jsonl\",\n",
    "    \"F:\\\\applied_deep_learning\\\\data\\\\original_M4_data\\\\peerread_llama.jsonl\"\n",
    "]\n",
    "\n",
    "# load each file into a separate dataframe and name them according to the files\n",
    "df_chatgpt = process_json_file(file_paths[1])\n",
    "df_cohere = process_json_file(file_paths[2])\n",
    "df_davinci = process_json_file(file_paths[3])\n",
    "df_dolly = process_json_file(file_paths[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8142, 2)\n",
      "(9928, 2)\n",
      "(8142, 2)\n",
      "(9928, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_chatgpt.shape)\n",
    "print(df_cohere.shape)\n",
    "print(df_davinci.shape)\n",
    "print(df_dolly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>human_reviews</th>\n",
       "      <th>davinci_reviews</th>\n",
       "      <th>chatgpt_reviews</th>\n",
       "      <th>prompts</th>\n",
       "      <th>bloom_reviews</th>\n",
       "      <th>score</th>\n",
       "      <th>probas</th>\n",
       "      <th>logits</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>Bridge Text and Knowledge by Learning Multi-Pr...</td>\n",
       "      <td>Integrating text and knowledge into a unified ...</td>\n",
       "      <td>[- Strengths:\\n* Outperforms ALIGN in supervis...</td>\n",
       "      <td>[\\n\\nThis paper presents a novel approach to b...</td>\n",
       "      <td>[Title: Bridge Text and Knowledge by Learning ...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose to learn multi-prototype ...</td>\n",
       "      <td>0.268284</td>\n",
       "      <td>[0.4606933594, 0.1846923828, 0.1619873047, 0.3...</td>\n",
       "      <td>[19.46875, 17.359375, 18.515625, 22.15625, 18....</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>Morphological Inflection Generation with Hard ...</td>\n",
       "      <td>We present a neural model for morphological in...</td>\n",
       "      <td>[- Strengths:\\nThe idea of hard monotonic atte...</td>\n",
       "      <td>[\\n\\nThis paper presents a novel approach to m...</td>\n",
       "      <td>[Title: Morphological Inflection Generation wi...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose an attention mechanism to...</td>\n",
       "      <td>0.208110</td>\n",
       "      <td>[0.4658203125, 0.2700195312, 0.1539306641, 0.2...</td>\n",
       "      <td>[19.125, 17.234375, 18.25, 20.1875, 16.625, 19...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>107</td>\n",
       "      <td>Weakly Supervised Cross-Lingual Named Entity R...</td>\n",
       "      <td>The state-of-the-art named entity recognition ...</td>\n",
       "      <td>[This paper presents several weakly supervised...</td>\n",
       "      <td>[\\n\\nThis paper presents a novel approach to w...</td>\n",
       "      <td>[General Comments:\\nThe paper titled \"Weakly S...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose an effective method to re...</td>\n",
       "      <td>0.254676</td>\n",
       "      <td>[0.4738769531, 0.3132324219, 0.2371826172, 0.4...</td>\n",
       "      <td>[18.953125, 21.234375, 19.78125, 21.875, 20.57...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>108</td>\n",
       "      <td>A Multigraph-based Model for Overlapping Entit...</td>\n",
       "      <td>In this paper, we propose a new model for pred...</td>\n",
       "      <td>[- Strengths: the paper is well-written, excep...</td>\n",
       "      <td>[\\n\\nThis paper proposes a multigraph-based mo...</td>\n",
       "      <td>[\\n\\nOverview:\\nThe paper proposes a multigrap...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors present an interesting approach t...</td>\n",
       "      <td>0.299974</td>\n",
       "      <td>[0.4094238281, 0.2763671875, 0.1287841797, 0.1...</td>\n",
       "      <td>[19.03125, 18.046875, 18.15625, 19.171875, 26....</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>117</td>\n",
       "      <td>Improved Neural Relation Detection for Knowled...</td>\n",
       "      <td>Relation detection is a core component of many...</td>\n",
       "      <td>[- Strengths: The paper addresses a relevant t...</td>\n",
       "      <td>[\\n\\nThis paper presents a novel approach for ...</td>\n",
       "      <td>[\\n\\nTitle: Improved Neural Relation Detection...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors present an improved neural relati...</td>\n",
       "      <td>0.266504</td>\n",
       "      <td>[0.4230957031, 0.3024902344, 0.1875, 0.3144531...</td>\n",
       "      <td>[19.28125, 17.953125, 18.6875, 22.21875, 18.40...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source   id                                              title  \\\n",
       "0  PeerRead/acl_2017/  104  Bridge Text and Knowledge by Learning Multi-Pr...   \n",
       "1  PeerRead/acl_2017/  105  Morphological Inflection Generation with Hard ...   \n",
       "2  PeerRead/acl_2017/  107  Weakly Supervised Cross-Lingual Named Entity R...   \n",
       "3  PeerRead/acl_2017/  108  A Multigraph-based Model for Overlapping Entit...   \n",
       "4  PeerRead/acl_2017/  117  Improved Neural Relation Detection for Knowled...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Integrating text and knowledge into a unified ...   \n",
       "1  We present a neural model for morphological in...   \n",
       "2  The state-of-the-art named entity recognition ...   \n",
       "3  In this paper, we propose a new model for pred...   \n",
       "4  Relation detection is a core component of many...   \n",
       "\n",
       "                                       human_reviews  \\\n",
       "0  [- Strengths:\\n* Outperforms ALIGN in supervis...   \n",
       "1  [- Strengths:\\nThe idea of hard monotonic atte...   \n",
       "2  [This paper presents several weakly supervised...   \n",
       "3  [- Strengths: the paper is well-written, excep...   \n",
       "4  [- Strengths: The paper addresses a relevant t...   \n",
       "\n",
       "                                     davinci_reviews  \\\n",
       "0  [\\n\\nThis paper presents a novel approach to b...   \n",
       "1  [\\n\\nThis paper presents a novel approach to m...   \n",
       "2  [\\n\\nThis paper presents a novel approach to w...   \n",
       "3  [\\n\\nThis paper proposes a multigraph-based mo...   \n",
       "4  [\\n\\nThis paper presents a novel approach for ...   \n",
       "\n",
       "                                     chatgpt_reviews  \\\n",
       "0  [Title: Bridge Text and Knowledge by Learning ...   \n",
       "1  [Title: Morphological Inflection Generation wi...   \n",
       "2  [General Comments:\\nThe paper titled \"Weakly S...   \n",
       "3  [\\n\\nOverview:\\nThe paper proposes a multigrap...   \n",
       "4  [\\n\\nTitle: Improved Neural Relation Detection...   \n",
       "\n",
       "                                             prompts  \\\n",
       "0  [Please write a peer review for the paper give...   \n",
       "1  [Please write a peer review for the paper give...   \n",
       "2  [Please write a peer review for the paper give...   \n",
       "3  [Please write a peer review for the paper give...   \n",
       "4  [Please write a peer review for the paper give...   \n",
       "\n",
       "                                       bloom_reviews     score  \\\n",
       "0  [The authors propose to learn multi-prototype ...  0.268284   \n",
       "1  [The authors propose an attention mechanism to...  0.208110   \n",
       "2  [The authors propose an effective method to re...  0.254676   \n",
       "3  [The authors present an interesting approach t...  0.299974   \n",
       "4  [The authors present an improved neural relati...  0.266504   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [0.4606933594, 0.1846923828, 0.1619873047, 0.3...   \n",
       "1  [0.4658203125, 0.2700195312, 0.1539306641, 0.2...   \n",
       "2  [0.4738769531, 0.3132324219, 0.2371826172, 0.4...   \n",
       "3  [0.4094238281, 0.2763671875, 0.1287841797, 0.1...   \n",
       "4  [0.4230957031, 0.3024902344, 0.1875, 0.3144531...   \n",
       "\n",
       "                                              logits              model  \n",
       "0  [19.46875, 17.359375, 18.515625, 22.15625, 18....  bigscience/bloomz  \n",
       "1  [19.125, 17.234375, 18.25, 20.1875, 16.625, 19...  bigscience/bloomz  \n",
       "2  [18.953125, 21.234375, 19.78125, 21.875, 20.57...  bigscience/bloomz  \n",
       "3  [19.03125, 18.046875, 18.15625, 19.171875, 26....  bigscience/bloomz  \n",
       "4  [19.28125, 17.953125, 18.6875, 22.21875, 18.40...  bigscience/bloomz  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do it for the bloomz file which has a slightly different format\n",
    "\n",
    "df_bloomz = pd.read_json(file_paths[0], lines=True)\n",
    "df_bloomz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The authors propose to learn multi-prototype e...</td>\n",
       "      <td>bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The main idea of this work is to learn multi-p...</td>\n",
       "      <td>bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The authors present their work very clearly.  ...</td>\n",
       "      <td>bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The authors present their work very clearly.  ...</td>\n",
       "      <td>bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The authors propose an attention mechanism to ...</td>\n",
       "      <td>bloomz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   model\n",
       "0  The authors propose to learn multi-prototype e...  bloomz\n",
       "0  The main idea of this work is to learn multi-p...  bloomz\n",
       "0  The authors present their work very clearly.  ...  bloomz\n",
       "0  The authors present their work very clearly.  ...  bloomz\n",
       "1  The authors propose an attention mechanism to ...  bloomz"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explode 'bloom_reviews' and assign 'bloomz' to the 'model' column\n",
    "\n",
    "df_bloomz = df_bloomz[['bloom_reviews']].explode('bloom_reviews')\n",
    "df_bloomz.rename(columns={'bloom_reviews': 'text'}, inplace=True)\n",
    "df_bloomz['model'] = 'bloomz'\n",
    "df_bloomz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38480, 2)\n",
      "                                                text  model\n",
      "0  - Strengths:\\n* Outperforms ALIGN in supervise...  human\n",
      "1  This paper addresses the problem of disambigua...  human\n",
      "2  - Strengths:\\nGood ideas, simple neural learni...  human\n",
      "3  - Strengths:\\nThe idea of hard monotonic atten...  human\n",
      "4  - Strengths: A new encoder-decoder model is pr...  human\n"
     ]
    }
   ],
   "source": [
    "# lets concat all the dataframes into one\n",
    "\n",
    "df_merge = pd.concat([df_chatgpt, df_cohere, df_davinci, df_dolly, df_bloomz], ignore_index=True)\n",
    "\n",
    "print(df_merge.shape)\n",
    "print(df_merge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14566, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets drop the duplicates in the text column\n",
    "\n",
    "df_merge = df_merge.drop_duplicates(subset=['text'])\n",
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in train_df: ['chatGPT' 'human' 'cohere' 'davinci' 'bloomz' 'dolly']\n",
      "Unique classes in dev_df: ['chatGPT' 'human' 'davinci' 'cohere' 'bloomz' 'dolly']\n",
      "Unique classes in df_merge: ['human' 'chatgpt' 'cohere' 'davinci' 'dolly-v2' 'bloomz']\n"
     ]
    }
   ],
   "source": [
    "# lets check if the classes are named the same in all the dataframes\n",
    "\n",
    "unique_classes_train = train_df['model'].unique()\n",
    "unique_classes_dev = dev_df['model'].unique()\n",
    "unique_classes_merge = df_merge['model'].unique()\n",
    "\n",
    "print(\"Unique classes in train_df:\", unique_classes_train)\n",
    "print(\"Unique classes in dev_df:\", unique_classes_dev)\n",
    "print(\"Unique classes in df_merge:\", unique_classes_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in df_merge: ['human' 'chatGPT' 'cohere' 'davinci' 'dolly' 'bloomz']\n"
     ]
    }
   ],
   "source": [
    "# lets map the classes to the same name\n",
    "    \n",
    "class_mapping = {\n",
    "    'chatgpt': 'chatGPT',\n",
    "    'davinci': 'davinci',\n",
    "    'cohere': 'cohere',\n",
    "    'human': 'human',\n",
    "    'bloomz': 'bloomz',\n",
    "    'dolly-v2': 'dolly'\n",
    "}\n",
    "\n",
    "df_merge['model'] = df_merge['model'].map(class_mapping)\n",
    "\n",
    "print(\"Unique classes in df_merge:\", df_merge['model'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model    label\n",
      "bloomz   4        500\n",
      "chatGPT  1        500\n",
      "cohere   2        500\n",
      "davinci  3        500\n",
      "dolly    5        500\n",
      "human    0        500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# group by 'model' and 'label' from the original test set to see the mappings of the labels and the models\n",
    "model_label_mapping = dev_df.groupby(['model', 'label']).size()\n",
    "\n",
    "print(model_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary mapping for labels\n",
    "label_mapping = {\n",
    "    'human': 0,\n",
    "    'chatGPT': 1,\n",
    "    'cohere': 2,\n",
    "    'davinci': 3,\n",
    "    'bloomz': 4,\n",
    "    'dolly': 5\n",
    "}\n",
    "\n",
    "# map the 'model' column to labels using the dictionary mapping\n",
    "df_merge['label'] = df_merge['model'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model    label\n",
      "bloomz   4        2334\n",
      "chatGPT  1        2344\n",
      "cohere   2        2342\n",
      "davinci  3        2344\n",
      "dolly    5        2344\n",
      "human    0        2858\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# lets check the mapping\n",
    "\n",
    "model_label_mapping_merge = df_merge.groupby(['model', 'label']).size()\n",
    "\n",
    "print(model_label_mapping_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  model  label\n",
      "0  - Strengths:\\n* Outperforms ALIGN in supervise...  human      0\n",
      "1  This paper addresses the problem of disambigua...  human      0\n",
      "2  - Strengths:\\nGood ideas, simple neural learni...  human      0\n",
      "3  - Strengths:\\nThe idea of hard monotonic atten...  human      0\n",
      "4  - Strengths: A new encoder-decoder model is pr...  human      0\n",
      "                                                text    model    source  \\\n",
      "0  Overall, I found the paper \"Machine Comprehens...  chatGPT  peerread   \n",
      "1  This paper \"Machine Comprehension Using Match-...  chatGPT  peerread   \n",
      "2  The paper presents an end-to-end neural archit...  chatGPT  peerread   \n",
      "3  This paper proposes an end-to-end neural archi...  chatGPT  peerread   \n",
      "4  Title: Incorporating long-range consistency in...  chatGPT  peerread   \n",
      "\n",
      "   label    id  \n",
      "0      1  1844  \n",
      "1      1  1845  \n",
      "2      1  1846  \n",
      "3      1  1847  \n",
      "4      1  1848  \n"
     ]
    }
   ],
   "source": [
    "# lets compare the dataframe \n",
    "\n",
    "print(df_merge.head())\n",
    "print(dev_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Strengths:\\n* Outperforms ALIGN in supervise...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>peerread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper addresses the problem of disambigua...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>peerread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Strengths:\\nGood ideas, simple neural learni...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>peerread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Strengths:\\nThe idea of hard monotonic atten...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>peerread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Strengths: A new encoder-decoder model is pr...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>peerread</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  model  label    source\n",
       "0  - Strengths:\\n* Outperforms ALIGN in supervise...  human      0  peerread\n",
       "1  This paper addresses the problem of disambigua...  human      0  peerread\n",
       "2  - Strengths:\\nGood ideas, simple neural learni...  human      0  peerread\n",
       "3  - Strengths:\\nThe idea of hard monotonic atten...  human      0  peerread\n",
       "4  - Strengths: A new encoder-decoder model is pr...  human      0  peerread"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets head the the source column into the df_merge dataframe\n",
    "df_merge['source'] = \"peerread\"\n",
    "\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we now have the full dataset of the Peerread domain available, we can make new splits to use!\n",
    "\n",
    "So for the first we will split the provided trainset into train data and validation data. we have 71k datapoint and split 90:10 train:validation. our test set whas a fixed size of 14.500 datapoints. so we get a ~ 75:8:17 split.\n",
    "\n",
    "For the second new split we will mix all domains into train:validation:test with an equal distribuation of models and domains in every split with a 80:10:10 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_set_comp: (63924, 5)\n",
      "Shape of val_set_comp: (7103, 5)\n",
      "Shape of test_set_comp: (14566, 4)\n",
      "Train set class distribution:\n",
      " label\n",
      "3    0.168935\n",
      "4    0.168919\n",
      "0    0.168904\n",
      "1    0.168888\n",
      "5    0.164758\n",
      "2    0.159596\n",
      "Name: proportion, dtype: float64\n",
      "Validation set class distribution:\n",
      " label\n",
      "4    0.168943\n",
      "0    0.168943\n",
      "3    0.168943\n",
      "1    0.168802\n",
      "5    0.164719\n",
      "2    0.159651\n",
      "Name: proportion, dtype: float64\n",
      "Test set class distribution:\n",
      " label\n",
      "0    0.196210\n",
      "1    0.160923\n",
      "3    0.160923\n",
      "5    0.160923\n",
      "2    0.160785\n",
      "4    0.160236\n",
      "Name: proportion, dtype: float64\n",
      "Train set source distribution:\n",
      " source\n",
      "reddit       0.253410\n",
      "wikihow      0.253410\n",
      "arxiv        0.253379\n",
      "wikipedia    0.239800\n",
      "Name: proportion, dtype: float64\n",
      "Validation set source distribution:\n",
      " source\n",
      "reddit       0.253414\n",
      "arxiv        0.253414\n",
      "wikihow      0.253414\n",
      "wikipedia    0.239758\n",
      "Name: proportion, dtype: float64\n",
      "Test set source distribution:\n",
      " source\n",
      "peerread    1.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# First dataset which is the one for the competition\n",
    "\n",
    "# split the train data into train and validation sets\n",
    "# as we want stratification along the domain an model columns we will combine them into one column as train_test_split function can only take one column as input\n",
    "train_df['stratify_col'] = train_df['model'].astype(str) + \"_\" + train_df['source'].astype(str)\n",
    "\n",
    "# split the data stratifying on the new column\n",
    "train_set_comp, val_set_comp = train_test_split(train_df, stratify=train_df['stratify_col'], test_size=0.1, random_state=42)\n",
    "\n",
    "# drop 'stratify_col' from both dataframes, no longer needed\n",
    "train_set_comp = train_set_comp.drop('stratify_col', axis=1)\n",
    "val_set_comp = val_set_comp.drop('stratify_col', axis=1)\n",
    "\n",
    "# now we copy the new constructe \"peerread\" dataset as test set\n",
    "test_set_comp = df_merge.copy()\n",
    "\n",
    "# lets check the shape of the datasets\n",
    "print(\"Shape of train_set_comp:\", train_set_comp.shape)\n",
    "print(\"Shape of val_set_comp:\", val_set_comp.shape)\n",
    "print(\"Shape of test_set_comp:\", test_set_comp.shape)\n",
    "\n",
    "# lets check the distribution of the model classes in the train, validation and test sets\n",
    "print(\"Train set class distribution:\\n\", train_set_comp['label'].value_counts(normalize=True))\n",
    "print(\"Validation set class distribution:\\n\", val_set_comp['label'].value_counts(normalize=True))\n",
    "print(\"Test set class distribution:\\n\", test_set_comp['label'].value_counts(normalize=True))\n",
    "\n",
    "# lets check the distribution of the source classes in the train, validation and test sets\n",
    "print(\"Train set source distribution:\\n\", train_set_comp['source'].value_counts(normalize=True))\n",
    "print(\"Validation set source distribution:\\n\", val_set_comp['source'].value_counts(normalize=True))\n",
    "print(\"Test set source distribution:\\n\", test_set_comp['source'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set nan values:\n",
      " text      0\n",
      "model     0\n",
      "source    0\n",
      "label     0\n",
      "id        0\n",
      "dtype: int64\n",
      "Validation set nan values:\n",
      " text      0\n",
      "model     0\n",
      "source    0\n",
      "label     0\n",
      "id        0\n",
      "dtype: int64\n",
      "Test set nan values:\n",
      " text      0\n",
      "model     0\n",
      "label     0\n",
      "source    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# lets check for nan values in the train, validation and test sets\n",
    "print(\"Train set nan values:\\n\", train_set_comp.isna().sum())\n",
    "print(\"Validation set nan values:\\n\", val_set_comp.isna().sum())\n",
    "print(\"Test set nan values:\\n\", test_set_comp.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_test_set: (85593, 6)\n",
      "Head the first rows of the train_test_set:\n",
      "                                                 text    model   source  label  \\\n",
      "0  Forza Motorsport is a popular racing game that...  chatGPT  wikihow      1   \n",
      "1  Buying Virtual Console games for your Nintendo...  chatGPT  wikihow      1   \n",
      "2  Windows NT 4.0 was a popular operating system ...  chatGPT  wikihow      1   \n",
      "3  How to Make Perfume\\n\\nPerfume is a great way ...  chatGPT  wikihow      1   \n",
      "4  How to Convert Song Lyrics to a Song'\\n\\nConve...  chatGPT  wikihow      1   \n",
      "\n",
      "    id     stratify_col  \n",
      "0  0.0  chatGPT_wikihow  \n",
      "1  1.0  chatGPT_wikihow  \n",
      "2  2.0  chatGPT_wikihow  \n",
      "3  3.0  chatGPT_wikihow  \n",
      "4  4.0  chatGPT_wikihow  \n"
     ]
    }
   ],
   "source": [
    "# lets make the second dataset with different split an stratification\n",
    "# first lets combine train and test set into one dataframe\n",
    "train_test_set = pd.concat([train_df, df_merge], ignore_index=True)\n",
    "\n",
    "# lets check the shape of the datasets\n",
    "print(\"Shape of train_test_set:\", train_test_set.shape)\n",
    "print('Head the first rows of the train_test_set:\\n', train_test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test set nan values:\n",
      " text            0\n",
      "model           0\n",
      "source          0\n",
      "label           0\n",
      "id              0\n",
      "stratify_col    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# we have NAN values in the id and straify_col columns\n",
    "# lets fill the id column with the index values and make a new column for the stratify_col\n",
    "train_test_set['id'] = train_test_set.index\n",
    "train_test_set['stratify_col'] = train_test_set['model'].astype(str) + \"_\" + train_test_set['source'].astype(str)\n",
    "\n",
    "# lets check for nan values in the train test set  \n",
    "print(\"Train test set nan values:\\n\", train_test_set.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_set_mix: (68474, 5)\n",
      "Shape of val_set_mix: (8559, 5)\n",
      "Shape of test_set_mix: (8560, 5)\n",
      "Train set class distribution:\n",
      " label\n",
      "0    0.173555\n",
      "3    0.167567\n",
      "1    0.167523\n",
      "4    0.167436\n",
      "5    0.164106\n",
      "2    0.159812\n",
      "Name: proportion, dtype: float64\n",
      "Validation set class distribution:\n",
      " label\n",
      "0    0.173502\n",
      "3    0.167660\n",
      "4    0.167543\n",
      "1    0.167426\n",
      "5    0.164038\n",
      "2    0.159832\n",
      "Name: proportion, dtype: float64\n",
      "Test set class distribution:\n",
      " label\n",
      "0    0.173598\n",
      "1    0.167640\n",
      "3    0.167523\n",
      "4    0.167407\n",
      "5    0.164136\n",
      "2    0.159696\n",
      "Name: proportion, dtype: float64\n",
      "Train set source distribution:\n",
      " source\n",
      "wikihow      0.210284\n",
      "reddit       0.210284\n",
      "arxiv        0.210255\n",
      "wikipedia    0.198995\n",
      "peerread     0.170181\n",
      "Name: proportion, dtype: float64\n",
      "Validation set source distribution:\n",
      " source\n",
      "reddit       0.210305\n",
      "arxiv        0.210305\n",
      "wikihow      0.210305\n",
      "wikipedia    0.198972\n",
      "peerread     0.170113\n",
      "Name: proportion, dtype: float64\n",
      "Test set source distribution:\n",
      " source\n",
      "wikihow      0.210280\n",
      "arxiv        0.210280\n",
      "reddit       0.210280\n",
      "wikipedia    0.198949\n",
      "peerread     0.170210\n",
      "Name: proportion, dtype: float64\n",
      "Train set nan values:\n",
      " text      0\n",
      "model     0\n",
      "source    0\n",
      "label     0\n",
      "id        0\n",
      "dtype: int64\n",
      "Validation set nan values:\n",
      " text      0\n",
      "model     0\n",
      "source    0\n",
      "label     0\n",
      "id        0\n",
      "dtype: int64\n",
      "Test set nan values:\n",
      " text      0\n",
      "model     0\n",
      "source    0\n",
      "label     0\n",
      "id        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# lest split the train test set into the mixed domain 70:15:15 split\n",
    "train_set_mix, val_set_mix = train_test_split(train_test_set, stratify=train_test_set['stratify_col'], test_size=0.20, random_state=42)\n",
    "val_set_mix, test_set_mix = train_test_split(val_set_mix, stratify=val_set_mix['stratify_col'], test_size=0.50, random_state=42)\n",
    "\n",
    "# drop 'stratify_col' from all dataframes, no longer needed\n",
    "train_set_mix = train_set_mix.drop('stratify_col', axis=1)\n",
    "val_set_mix = val_set_mix.drop('stratify_col', axis=1)\n",
    "test_set_mix = test_set_mix.drop('stratify_col', axis=1)\n",
    "\n",
    "# lets check the shape of the datasets\n",
    "print(\"Shape of train_set_mix:\", train_set_mix.shape)\n",
    "print(\"Shape of val_set_mix:\", val_set_mix.shape)\n",
    "print(\"Shape of test_set_mix:\", test_set_mix.shape)\n",
    "\n",
    "# lets check the distribution of the model classes in the train, validation and test sets\n",
    "print(\"Train set class distribution:\\n\", train_set_mix['label'].value_counts(normalize=True))\n",
    "print(\"Validation set class distribution:\\n\", val_set_mix['label'].value_counts(normalize=True))\n",
    "print(\"Test set class distribution:\\n\", test_set_mix['label'].value_counts(normalize=True))\n",
    "\n",
    "# lets check the distribution of the source classes in the train, validation and test sets\n",
    "print(\"Train set source distribution:\\n\", train_set_mix['source'].value_counts(normalize=True))\n",
    "print(\"Validation set source distribution:\\n\", val_set_mix['source'].value_counts(normalize=True))\n",
    "print(\"Test set source distribution:\\n\", test_set_mix['source'].value_counts(normalize=True))\n",
    "\n",
    "# lets check for nan values in the train, validation and test sets\n",
    "print(\"Train set nan values:\\n\", train_set_mix.isna().sum())\n",
    "print(\"Validation set nan values:\\n\", val_set_mix.isna().sum())\n",
    "print(\"Test set nan values:\\n\", test_set_mix.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the datasets as jsonl files\n",
    "train_set_comp.to_json('data/train_set_comp.jsonl', orient='records', lines=True)\n",
    "val_set_comp.to_json('data/val_set_comp.jsonl', orient='records', lines=True)\n",
    "test_set_comp.to_json('data/test_set_comp.jsonl', orient='records', lines=True)\n",
    "\n",
    "train_set_mix.to_json('data/train_set_mix.jsonl', orient='records', lines=True)\n",
    "val_set_mix.to_json('data/val_set_mix.jsonl', orient='records', lines=True)\n",
    "test_set_mix.to_json('data/test_set_mix.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
